{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600875749685",
   "display_name": "Python 3.8.2 64-bit ('kerasenv': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset.txt', 'r') as f:\n",
    "        raw_data = f.read().lower()\n",
    "        \n",
    "data = raw_data.replace(\"<eos>\",\"\\n\")\n",
    "\n",
    "print(\"Corpus length: %d characters\" % len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sonnet character length\n",
    "sonnets = data.split('\\n\\n')\n",
    "sonnet_lens = [len(sonnet) for sonnet in sonnets]\n",
    "\n",
    "print('Average sonnet length: %.2f characters' % np.mean(sonnet_lens))\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.bar([i for i in range(1, len(sonnets)+1)], sonnet_lens)\n",
    "plt.title('Number of Characters per sonnet')\n",
    "plt.ylabel('# Characters')\n",
    "plt.xlabel('Sonnets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max length of each sequence\n",
    "maxlen = 45\n",
    "\n",
    "# Sample new sequence every step characters\n",
    "step = 3\n",
    "\n",
    "sentences = []\n",
    "targets = []\n",
    "\n",
    "# Loop through sonnets and create sequences and associated targets\n",
    "for i in range(0, len(data) - maxlen, step):\n",
    "    sentences.append(data[i:i + maxlen])\n",
    "    targets.append(data[maxlen + i])\n",
    "\n",
    "print(\"Number of sequences:\", len(sentences))\n",
    "\n",
    "# Grab all unique characters in corpus\n",
    "chars = sorted(list(set(data)))\n",
    "print(\"Number of unique characters:\", len(chars))\n",
    "\n",
    "# Dictionary mapping unique character to integer indices\n",
    "char_indices = dict((char, chars.index(char)) for char in chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize sequences and targets\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for j, char in enumerate(sentence):\n",
    "        x[i, j, char_indices[char]] = 1\n",
    "    y[i, char_indices[targets[i]]] = 1\n",
    "\n",
    "print(\"Size of training sequences:\", x.shape)\n",
    "print(\"Size of training targets:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "\n",
    "optimizer = SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    ''' Reweight the predicted probabilities and draw sample from newly created probability distribution. '''\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 60\n",
    "\n",
    "loss = []  # Custom history list to save model's loss\n",
    "\n",
    "# Create directory to store generated text\n",
    "base_dir = 'generated_text'\n",
    "if not os.path.isdir(base_dir):\n",
    "    os.mkdir(base_dir)\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    print(\"Epoch\", epoch)\n",
    "    # Fit model for 1 epoch then generate text given a seed.\n",
    "    history = model.fit(x, y, batch_size=128, epochs=1)\n",
    "    loss.append(history.history['loss'][0])\n",
    "    \n",
    "    # Create directory to store text for each epoch\n",
    "    epoch_dir = os.path.join(base_dir, 'epoch_' + str(epoch))\n",
    "    if not os.path.isdir(epoch_dir):\n",
    "        os.mkdir(epoch_dir)\n",
    "    \n",
    "    # Select a random seed text to feed into model and generate text\n",
    "    start_idx = np.random.randint(0, len(data) - maxlen - 1)\n",
    "    seed_text = data[start_idx:start_idx + maxlen]\n",
    "    for temp in [0.2, 0.5, 1.0, 1.3]:\n",
    "        generated_text = seed_text\n",
    "        temp_file = 'epoch' + str(epoch) + '_temp' + str(temp) + '.txt'\n",
    "        file = open(os.path.join(epoch_dir, temp_file), 'w')\n",
    "        file.write(generated_text)\n",
    "        \n",
    "        # Predict and generate 600 characters (approx. 1 sonnet length)\n",
    "        for i in range(630):\n",
    "            # Vectorize generated text\n",
    "            sampled = np.zeros((1, maxlen, len(chars)))\n",
    "            for j, char in enumerate(generated_text):\n",
    "                sampled[0, j, char_indices[char]] = 1.\n",
    "            \n",
    "            # Predict next character\n",
    "            preds = model.predict(sampled, verbose=0)[0]\n",
    "            pred_idx = sample(preds, temperature=temp)\n",
    "            next_char = chars[pred_idx]\n",
    "            \n",
    "            # Append predicted character to seed text\n",
    "            generated_text += next_char\n",
    "            generated_text = generated_text[1:]\n",
    "            \n",
    "            # Write to text file\n",
    "            file.write(next_char)\n",
    "        print('Temp ' + str(temp) + \" done.\")\n",
    "        file.close()"
   ]
  },
  {
   "source": [
    "## Generating New Sonnets\n",
    "Here we will pick a random seed text from the training data and predict 630 (average sonnet length) new characters using our newly trained model. We will also use a temperature of 0.5 because that gives a good balance between randomness and deterministic behaviour."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sonnet(temp):\n",
    "    ''' Given a temperature, generate a new sonnet '''\n",
    "    start_idx = np.random.randint(0, len(data) - maxlen - 1)\n",
    "    new_sonnet = data[start_idx:start_idx + maxlen]\n",
    "    sys.stdout.write(new_sonnet)\n",
    "    for i in range(600):\n",
    "        # Vectorize generated text\n",
    "        sampled = np.zeros((1, maxlen, len(chars)))\n",
    "        for j, char in enumerate(new_sonnet):\n",
    "            sampled[0, j, char_indices[char]] = 1.\n",
    "\n",
    "        # Predict next character\n",
    "        preds = model.predict(sampled, verbose=0)[0]\n",
    "        pred_idx = sample(preds, temperature=temp)\n",
    "        next_char = chars[pred_idx]\n",
    "\n",
    "        # Append predicted character to seed text\n",
    "        new_sonnet += next_char\n",
    "        new_sonnet = new_sonnet[1:]\n",
    "\n",
    "        # Print to console\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new sonnets at 0.5 temperature\n",
    "generate_sonnet(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save('sonnet_model.h5')"
   ]
  }
 ]
}